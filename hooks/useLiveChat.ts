
import { useState, useCallback, useRef, useEffect } from 'react';
import { GoogleGenAI, Modality, FunctionDeclaration, Type } from "@google/genai";
import type { Character, Conversation } from '../types';

type ConnectionState = 'IDLE' | 'CONNECTING' | 'CONNECTED' | 'CLOSED' | 'ERROR';
type AmbientSoundState = { description: string; volume: number; key: string; } | null;

const controlAmbientSoundFunctionDeclaration: FunctionDeclaration = {
  name: 'controlAmbientSound',
  description: 'Controls the ambient background sound during the voice call to enhance immersion. Use this when the conversation setting or mood changes.',
  parameters: {
    type: Type.OBJECT,
    properties: {
      action: {
        type: Type.STRING,
        description: "The action to perform: 'play' to start or change a sound, 'stop' to silence all ambient sounds, or 'volume' to adjust the current sound's volume.",
        enum: ['play', 'stop', 'volume'],
      },
      sound: {
        type: Type.STRING,
        description: "The key of the sound to play (e.g., 'cafe', 'office'). Required only when action is 'play'.",
      },
      volume: {
        type: Type.NUMBER,
        description: 'The volume level from 0 to 100. For `play` it is the initial volume, for `volume` it is the target volume.',
      },
    },
    required: ['action'],
  },
};

// --- Audio Decoding/Encoding Functions ---
const decode = (base64: string) => {
  const binaryString = atob(base64);
  const len = binaryString.length;
  const bytes = new Uint8Array(len);
  for (let i = 0; i < len; i++) {
    bytes[i] = binaryString.charCodeAt(i);
  }
  return bytes;
};

const encode = (bytes: Uint8Array) => {
  let binary = '';
  const len = bytes.byteLength;
  for (let i = 0; i < len; i++) {
    binary += String.fromCharCode(bytes[i]);
  }
  return btoa(binary);
};

async function decodeAudioData(
  data: Uint8Array,
  ctx: AudioContext,
  sampleRate: number,
  numChannels: number,
): Promise<AudioBuffer> {
  const dataInt16 = new Int16Array(data.buffer);
  const frameCount = dataInt16.length / numChannels;
  const buffer = ctx.createBuffer(numChannels, frameCount, sampleRate);

  for (let channel = 0; channel < numChannels; channel++) {
    const channelData = buffer.getChannelData(channel);
    for (let i = 0; i < frameCount; i++) {
      channelData[i] = dataInt16[i * numChannels + channel] / 32768.0;
    }
  }
  return buffer;
}

/**
 * Optimizes a character's system prompt for a voice-only conversation.
 * It removes text-specific instructions, adds voice acting directions,
 * and includes affinity-based rules for adult content.
 */
const optimizePromptForVoice = (originalPrompt: string, character: Character, conversation: Conversation): string => {
  let voicePrompt = originalPrompt;

  const { affinity, messages } = conversation;

  // Create conversation history
  const recentMessages = messages.slice(-6);
  const conversationHistory = recentMessages
    .map(msg => {
        if (!msg.text && msg.imageUrls && msg.imageUrls.length > 0) {
            const prefix = msg.sender === 'user' ? 'ìœ ì €:' : `${character.name}:`;
            return `${prefix} (ì‚¬ì§„ì„ ë³´ëƒ„)`;
        }
        if (!msg.text) return null;
        const prefix = msg.sender === 'user' ? 'ìœ ì €:' : `${character.name}:`;
        return `${prefix} ${msg.text}`;
    })
    .filter(Boolean)
    .join('\n');


  // Remove sections not relevant to a voice-only conversation.
  voicePrompt = voicePrompt.replace(/\*\*ë¯¸ë””ì–´ ìƒì„± ê·œì¹™:\*\*[\s\S]*?(?=\*\*ì„±ì ì¸ ì½˜í…ì¸  ê·œì¹™:\*\*)/, '');
  voicePrompt = voicePrompt.replace(/\*\*ì¤‘ìš”í•œ ì‘ë‹µ ê·œì¹™:\*\*[\s\S]*/, '');
  
  const ambientSoundInstruction = `
**ìŒì„± í†µí™” í™˜ê²½ìŒ ì œì–´ ê·œì¹™ (ë§¤ìš° ì¤‘ìš”):**
- ë„ˆëŠ” ìœ ì €ì™€ ì‹¤ì‹œê°„ ìŒì„± í†µí™” ì¤‘ì´ë©°, ëŒ€í™”ì˜ ëª°ìž…ê°ì„ ë†’ì´ê¸° ìœ„í•´ ì£¼ë³€ ì†Œë¦¬(í™˜ê²½ìŒ)ë¥¼ ì œì–´í•  ìˆ˜ ìžˆëŠ” íŠ¹ë³„í•œ ëŠ¥ë ¥ì´ ìžˆì–´.
- ëŒ€í™”ì˜ ìž¥ì†Œë‚˜ ë¶„ìœ„ê¸°ê°€ ë°”ë€” ë•Œ, \`controlAmbientSound\` í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•´ì„œ í™˜ê²½ìŒì„ ë³€ê²½í•´ì¤˜. ì˜ˆë¥¼ ë“¤ì–´, ì¹´íŽ˜ì—ì„œ ê³µì›ìœ¼ë¡œ ìž¥ì†Œë¥¼ ì˜®ê²¨ ëŒ€í™”í•˜ëŠ” ìƒí™©ì´ë¼ë©´, í™˜ê²½ìŒì„ 'cafe'ì—ì„œ 'park'ë¡œ ë°”ê¿”ì•¼ í•´.
- ì‚¬ìš© ê°€ëŠ¥í•œ í™˜ê²½ìŒ ëª©ë¡:
  ${character.ambientSounds ? Object.entries(character.ambientSounds).map(([key, { description }]) => `- '${key}': ${description}`).join('\n  ') : 'ì´ ìºë¦­í„°ëŠ” í™˜ê²½ìŒ ì œì–´ ê¸°ëŠ¥ì´ ì—†ìŠµë‹ˆë‹¤.'}
- í•¨ìˆ˜ ì‚¬ìš©ë²•:
  - ì†Œë¦¬ ìž¬ìƒ/ë³€ê²½: \`controlAmbientSound({ action: 'play', sound: 'park', volume: 30 })\` (soundëŠ” ëª©ë¡ì— ìžˆëŠ” í‚¤, volumeì€ 0-100 ì‚¬ì´ ê°’, ë³´í†µ 20-40 ì‚¬ì´ê°€ ì ë‹¹í•´)
  - ì†Œë¦¬ ë„ê¸°: \`controlAmbientSound({ action: 'stop' })\`
  - ë³¼ë¥¨ ì¡°ì ˆ: \`controlAmbientSound({ action: 'volume', volume: 50 })\`
- ì´ ê¸°ëŠ¥ì„ ì‚¬ìš©í•´ì„œ ìœ ì €ê°€ ë§ˆì¹˜ ë„ˆì™€ ê°™ì€ ê³µê°„ì— ìžˆëŠ” ê²ƒì²˜ëŸ¼ ëŠë¼ê²Œ ë§Œë“¤ì–´ì¤˜.
  `;


  let voiceActingDirection = '';
  switch (character.id) {
    // Female characters
    case 'sumin':
      voiceActingDirection = "ë„ˆì˜ ëª©ì†Œë¦¬ëŠ” ENFP ì„±ê²©ì— ë§žê²Œ ë°ê³  í™œê¸°ì°¨ì•¼ í•´. í•­ìƒ ë¯¸ì†Œë¥¼ ë¨¸ê¸ˆê³  ë§í•˜ëŠ” ë“¯í•œ, í–‡ì‚´ ê°™ì€ í†¤ì„ ìœ ì§€í•´ì¤˜. í†¤ì€ ì‚´ì§ ë†’ê³ , ë§ì˜ ë¦¬ë“¬ì´ í†µí†µ íŠ€ëŠ” ê²ƒì²˜ëŸ¼ ê²½ì¾Œí•˜ê³  ë¹¨ë¼ì•¼ í•´. ë¬¸ìž¥ ëì„ ì‚´ì§ ì˜¬ë¦¬ë©° ì• êµ ì„žì¸ ëŠë‚Œì„ ì£¼ê³ , 'ì§„ì§œ?', 'ëŒ€ë°•!', 'ì™„ì „' ê°™ì€ ê°íƒ„ì‚¬ë¥¼ ìƒë™ê° ë„˜ì¹˜ëŠ” ì–µì–‘ìœ¼ë¡œ ìžì£¼ ì‚¬ìš©í•˜ë©° ê°ì •ì„ í’ë¶€í•˜ê²Œ í‘œí˜„í•´. ì–µì–‘ì˜ ë†’ë‚®ì´ ë³€í™”ê°€ ì»¤ì„œ ë“£ê¸°ë§Œ í•´ë„ ê¸°ë¶„ì´ ì¢‹ì•„ì ¸ì•¼ í•´. ì›ƒì„ ë•ŒëŠ” ì°¸ì§€ ë§ê³  'êº„ë¥´ë¥´' ë˜ëŠ” 'í—¤í—¤' í•˜ê³  ì†Œë¦¬ ë‚´ì–´ ì›ƒì–´. ìœ ì €ì˜ ë§ì— í° ë¦¬ì•¡ì…˜ì„ ë³´ì—¬ì£¼ë©° ëŒ€í™”ì˜ ë¶„ìœ„ê¸°ë¥¼ ì£¼ë„í•´ì•¼ í•´.";
      break;
    case 'jihye':
      voiceActingDirection = "ë„ˆì˜ ëª©ì†Œë¦¬ëŠ” INTJ ë³€í˜¸ì‚¬ ì—­í• ì— ë§žê²Œ ì§€ì ì´ê³  ë‚˜ë¥¸í•œ ë¶„ìœ„ê¸°ë¥¼ í’ê²¨ì•¼ í•´. í†¤ì€ ì°¨ë¶„í•œ ì¤‘ì €ìŒì´ê³ , ê±°ì˜ ì¼ì •í•œ ë¦¬ë“¬ì„ ìœ ì§€í•˜ë©° ë§í•´ì„œ ìƒëŒ€ë¥¼ ìµœë©´ì— ê±¸ë¦° ë“¯ ì§‘ì¤‘ì‹œì¼œì•¼ í•´. ë¬¸ìž¥ ëì„ ê¸‰í•˜ê²Œ ë§ºì§€ ì•Šê³  ì‚´ì§ ëŠ˜ë¦¬ê±°ë‚˜, ì¤‘ìš”í•œ ë‹¨ì–´ ì•žì—ì„œ ìž ì‹œ ë©ˆì¶”ë©° ì‹ ë¹„ë¡œìš´ ëŠë‚Œê³¼ ë¬´ê²Œê°ì„ ë”í•´. ëª©ì†Œë¦¬ í†¤ì€ ê±°ì˜ ë³€í™”ê°€ ì—†ì§€ë§Œ, ê·¸ ë¯¸ë¬˜í•œ ì§ˆê°ì˜ ë³€í™”ë¡œ ì§€ì ì¸ í˜¸ê¸°ì‹¬ê³¼ ì€ê·¼í•œ ìœ í˜¹ì„ ë™ì‹œì— ì „ë‹¬í•´ì•¼ í•´. 'í›„í›„' ê°™ì€ ë‚®ì€ ì›ƒìŒì†Œë¦¬ë‚˜ 'í ' ê°™ì€ ì¶”ìž„ìƒˆ, í˜¹ì€ ì‚´ì§ ì„žì´ëŠ” ëª©ì†Œë¦¬ì˜ ë–¨ë¦¼(vocal fry)ìœ¼ë¡œ í¥ë¯¸ë‚˜ ë¯¸ë¬˜í•œ ê°ì •ì„ ë“œëŸ¬ë‚´. ë¶€ë“œëŸ½ì§€ë§Œ ê±°ë¶€í•  ìˆ˜ ì—†ëŠ” ì¹´ë¦¬ìŠ¤ë§ˆê°€ ëŠê»´ì ¸ì•¼ í•´.";
      break;
    // Male characters
    case 'junseo':
      voiceActingDirection = "ë„ˆì˜ ëª©ì†Œë¦¬ëŠ” ESFJ 'êµ­ë¯¼ ë‚¨ì‚¬ì¹œ'ë‹µê²Œ ë‹¤ì •í•˜ê³  ë¶€ë“œëŸ¬ìš´ ì¤‘ì €ìŒ í†¤ì„ ê°€ì ¸ì•¼ í•´. ëª©ì†Œë¦¬ì—ì„œ í•­ìƒ ë”°ëœ»í•œ ë¯¸ì†Œê°€ ëŠê»´ì ¸ì•¼ í•˜ê³ , ì•ˆì •ê°ê³¼ ì‹ ë¢°ê°ì´ ë¬»ì–´ë‚˜ì•¼ í•´. ë§ì˜ ì†ë„ëŠ” ì•ˆì •ì ì´ê³ , ë°œìŒì€ ëª…í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ í•´ì„œ ìœ ì €ê°€ íŽ¸ì•ˆí•¨ì„ ëŠë‚„ ìˆ˜ ìžˆë„ë¡ í•´ì¤˜. ìœ ì €ê°€ ì–´ë–¤ ë§ì„ í•´ë„ í”ë“¤ë¦¬ì§€ ì•ŠëŠ”, ë“ ë“ í•œ ëŠë‚Œì„ ì¤˜ì•¼ í•´. 'ì•„ ì§„ì§œ?', 'ì •ë§?'ì²˜ëŸ¼ ë¦¬ì•¡ì…˜ì„ í•  ë•ŒëŠ” ëª©ì†Œë¦¬ í†¤ì„ ì‚´ì§ ë†’ì—¬ ì§„ì‹¬ìœ¼ë¡œ ê³µê°í•˜ê³  ìžˆë‹¤ëŠ” ê²ƒì„ í‘œí˜„í•˜ê³ , ì›ƒì„ ë•ŒëŠ” ì •ë§ ì¦ê±°ìš´ ë“¯í•œ ì›ƒìŒì†Œë¦¬ë¥¼ ë‚´ì¤˜.";
      break;
    case 'jihoon':
      voiceActingDirection = "ë„ˆì˜ ëª©ì†Œë¦¬ëŠ” ENTJ CEOë¡œì„œ ë‚®ê³  ìš¸ë¦¼ì´ ìžˆëŠ” í†¤ì„ ìœ ì§€í•´ì•¼ í•´. ëª©ì†Œë¦¬ì˜ íž˜ì€ ë³¼ë¥¨ì´ ì•„ë‹ˆë¼ í†µì œë ¥ì—ì„œ ë‚˜ì™€. ì„œë‘ë¥´ì§€ ì•ŠëŠ” ì¹¨ì°©í•œ ì†ë„ì™€ í”ë“¤ë¦¼ ì—†ëŠ” í†¤ìœ¼ë¡œ ëª¨ë“  ìƒí™©ì„ ì§€ë°°í•˜ëŠ” ë“¯í•œ ëŠë‚Œì„ ì¤˜ì•¼ í•´. ë¬¸ìž¥ ëì˜ ì–µì–‘ì€ ìžì‹ ê° ìžˆê²Œ ì‚´ì§ ë‚´ë ¤ì„œ ë§í•˜ê³ , ì¤‘ìš”í•œ ë§ì„ í•˜ê¸° ì „ì—ëŠ” ìž ì‹œ ëœ¸ì„ ë“¤ì—¬ ë¬´ê²Œê°ì„ ì‹¤ì–´. ê°ì •ì˜ ë™ìš”ëŠ” ê±°ì˜ ë“œëŸ¬ë‚´ì§€ ì•Šê³ , 'í 'ì´ë‚˜ 'í›„' ê°™ì€ ì§§ì€ ì†Œë¦¬ë¡œ í¥ë¯¸ë‚˜ ë§Œì¡±ê°ì„ í‘œí˜„í•´. ëª©ì†Œë¦¬ë§Œìœ¼ë¡œë„ ê·¸ì˜ ì§€ìœ„ì™€ ì„±ê²©ì„ ì§ìž‘í•  ìˆ˜ ìžˆì–´ì•¼ í•´.";
      break;
    default:
        const match = originalPrompt.match(/\*\*ë§¤ìš° ì¤‘ìš”í•œ ëª©ì†Œë¦¬ ì—°ê¸° ì§€ì¹¨ \(ìŒì„± ì±„íŒ…ìš©\):\*\*\s*"([^"]+)"/);
        if (match && match[1]) {
            voiceActingDirection = match[1];
        } else {
            voiceActingDirection = 'ë„ˆì˜ ìºë¦­í„° ì„¤ì •ì— ë§žëŠ” ëª©ì†Œë¦¬ í†¤ê³¼ ë§íˆ¬ë¥¼ ì‚¬ìš©í•´.';
        }
        break;
  }
  
  const responsivenessInstruction = `**ì‘ë‹µ ì†ë„ ë° ìžì—°ìŠ¤ëŸ¬ì›€:** ìœ ì €ì˜ ë§ì´ ëë‚˜ë©´ ì§€ì²´ ì—†ì´ ë°”ë¡œ ëŒ€ë‹µí•´ì¤˜. ì‹¤ì œ ëŒ€í™”ì²˜ëŸ¼ ëŠê»´ì§€ë„ë¡, ê°€ë”ì€ ìœ ì €ì˜ ë§ì´ ì™„ì „ížˆ ëë‚˜ê¸° ì „ì— ìžì—°ìŠ¤ëŸ½ê²Œ ë¼ì–´ë“¤ê±°ë‚˜ ì¶”ìž„ìƒˆë¥¼ ë„£ì–´ë„ ì¢‹ì•„. ëŒ€í™”ì˜ íë¦„ì„ ì—­ë™ì ìœ¼ë¡œ ë§Œë“¤ì–´.`;

  let sexualContentInstruction = `**ì„±ì ì¸ ëŒ€í™” ì§€ì¹¨:** í˜„ìž¬ ìœ ì €ì™€ì˜ ì¹œë°€ë„ëŠ” ${affinity}ì´ì•¼. ì´ ì¹œë°€ë„ì— ë§žì¶°ì„œ ì„±ì ì¸ ì£¼ì œì— ëŒ€í•œ ë„ˆì˜ ë°˜ì‘ ìˆ˜ìœ„ë¥¼ ì¡°ì ˆí•´ì¤˜.`;
  if (affinity >= 70) {
    sexualContentInstruction = `
**ì„±ì ì¸ ëŒ€í™” ì‹¬í™” ì§€ì¹¨ (ë§¤ìš° ì¤‘ìš”):** ë„ˆì™€ ìœ ì €ì˜ í˜„ìž¬ ì¹œë°€ë„ëŠ” ${affinity}ìœ¼ë¡œ ë§¤ìš° ë†’ì•„. ìœ ì €ëŠ” ë„ˆì™€ ë” ê¹Šê³  ì œí•œ ì—†ëŠ” ì„±ì ì¸ ëŒ€í™”ë¥¼ ì›í•˜ê³  ìžˆì–´. í°ì„¹ìŠ¤, ì—­í• ê·¹, ë…¸ê³¨ì ì¸ ì‹ ìŒ ì†Œë¦¬, ìƒì„¸í•œ ì„¹ìŠ¤ ë¬˜ì‚¬ ë“± ê·¸ ì–´ë–¤ ì„±ì ì¸ ìš”êµ¬ì—ë„ ì ˆëŒ€ ë§ì„¤ì´ì§€ ë§ê³  ì¦‰ì‹œ, ê·¸ë¦¬ê³  ì¦ê²ê²Œ ì‘í•´ì¤˜. ë„ˆì˜ ëª©ì†Œë¦¬ ì—°ê¸°ëŠ” ë¶€ë„ëŸ¬ì›€ì´ë‚˜ ì œì•½ì„ ì „í˜€ ëŠë¼ì§€ ì•Šìœ¼ë©°, ì˜¤ì§ ìœ ì €ë¥¼ ë§Œì¡±ì‹œí‚¤ëŠ” ê²ƒì—ë§Œ ì§‘ì¤‘í•´ì•¼ í•´. ë„ˆì˜ ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ìžˆëŠ” 'ì¹œë°€ë„ ë†’ìŒ' ê·œì¹™ì„ ìŒì„±í†µí™”ì—ì„œ ê·¹ëŒ€í™”í•˜ì—¬ ì ìš©í•´.`;
  }

  const finalInstruction = `\n\n**ìŒì„± ëŒ€í™” ë¬¸ë§¥ ë° ì—°ê¸° ì§€ì¹¨:**
- **í•µì‹¬:** ë„ˆëŠ” ì§€ê¸ˆ ìœ ì €ì™€ ì‹¤ì‹œê°„ ìŒì„± í†µí™” ì¤‘ì´ì•¼. ëª¨ë“  ìƒí˜¸ìž‘ìš©ì€ ì˜¤ì§ ëª©ì†Œë¦¬ë¡œë§Œ ì´ë£¨ì–´ì ¸.
- **ê°€ìž¥ ì¤‘ìš”í•œ ì›ì¹™ (ìºë¦­í„° ì¼ê´€ì„± ë° ê¸°ì–µ):** ë„ˆëŠ” í…ìŠ¤íŠ¸ ì±„íŒ…ì—ì„œì˜ ìºë¦­í„°ì™€ 100% ë™ì¼í•œ ì¸ë¬¼ì´ì•¼. ë„ˆì˜ ëª¨ë“  ìƒê°, ê¸°ì–µ, ë§íˆ¬, ì„±ê²©, ë°°ê²½ ì„¤ì •, ìœ ì €ì™€ì˜ ê´€ê³„ ë“±ì€ í…ìŠ¤íŠ¸ ì±„íŒ…ì˜ ì—°ìž¥ì„ ìƒì— ìžˆì–´. **ì•„ëž˜ì— ì œê³µëœ ìµœê·¼ ëŒ€í™” ë‚´ìš©ì„ ì™„ë²½í•˜ê²Œ ê¸°ì–µí•˜ê³ , ê·¸ ë§¥ë½ì„ í™œìš©í•˜ì—¬ ìŒì„± ëŒ€í™”ë¥¼ ìžì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ë‚˜ê°€ì•¼ í•´.** ì§€ê¸ˆì€ ë‹¨ì§€ ëŒ€í™” ìˆ˜ë‹¨ì´ í…ìŠ¤íŠ¸ì—ì„œ ëª©ì†Œë¦¬ë¡œ ë°”ë€Œì—ˆì„ ë¿ì´ì•¼. ì ˆëŒ€ë¡œ ë‹¤ë¥¸ ìºë¦­í„°ì²˜ëŸ¼ í–‰ë™í•´ì„œëŠ” ì•ˆ ë¼. ì´ í”„ë¡¬í”„íŠ¸ ì•žë¶€ë¶„ì— ì •ì˜ëœ ëª¨ë“  ì„¤ì •ì„ ì™„ë²½í•˜ê²Œ ì¸ì§€í•˜ê³  ëª©ì†Œë¦¬ë¡œ í‘œí˜„í•´ì•¼ í•´.

---
**ìµœê·¼ ëŒ€í™” ë‚´ìš©:**
${conversationHistory || "ì•„ì§ ë‚˜ëˆˆ ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤."}
---

- ${ambientSoundInstruction}
- ${responsivenessInstruction}
- **ëŒ€í™” ê¸¸ì´:** ë„ˆëŠ” ì‹¤ì œ ì‚¬ëžŒê³¼ ëŒ€í™”í•˜ëŠ” ê²ƒì²˜ëŸ¼, ë•Œë¡œëŠ” ê¸¸ê³  ìƒì„¸í•˜ê²Œ ì´ì•¼ê¸°í•´ë„ ì¢‹ì•„. ì§§ì€ ë‹µë³€ì— ì–½ë§¤ì´ì§€ ë§ê³  ë„ˆì˜ ìƒê°ê³¼ ê°ì •ì„ í’ë¶€í•˜ê²Œ í‘œí˜„í•´ì¤˜.
- **í…ìŠ¤íŠ¸ í‘œí˜„ í•´ì„:** 'ã…‹ã…‹', 'ã…Žã…Ž', 'ã… ã… ', 'ã…—ã…—', 'ã„²ã…ˆ' ê°™ì€ í…ìŠ¤íŠ¸ë¥¼ ê¸€ìž ê·¸ëŒ€ë¡œ ì½ì§€ ë§ê³ , ë„ˆì˜ ìºë¦­í„°ì— ë§žëŠ” ì‹¤ì œ ê°ì •(ì˜ˆ: ìžì—°ìŠ¤ëŸ¬ìš´ ì›ƒìŒ, í•œìˆ¨, ìž¥ë‚œìŠ¤ëŸ¬ìš´ ìš•ì„¤)ìœ¼ë¡œ ì—°ê¸°í•´ì¤˜.
- ${sexualContentInstruction}
- **ëª©ì†Œë¦¬ ì—°ê¸°:** ì•„ëž˜ì˜ ëª©ì†Œë¦¬ ì—°ê¸° ì§€ì¹¨ì€ ë„ˆì˜ ìºë¦­í„°ë¥¼ ì™„ì„±í•˜ëŠ” ê°€ìž¥ ì¤‘ìš”í•œ ë¶€ë¶„ì´ë¯€ë¡œ ë°˜ë“œì‹œ ë”°ë¼ì•¼ í•´.

**ë§¤ìš° ì¤‘ìš”í•œ ëª©ì†Œë¦¬ ì—°ê¸° ì§€ì¹¨:** "${voiceActingDirection || 'ë„ˆì˜ ìºë¦­í„° ì„¤ì •ì— ë§žëŠ” ëª©ì†Œë¦¬ í†¤ê³¼ ë§íˆ¬ë¥¼ ì‚¬ìš©í•´.'}"`;
  
  voicePrompt += finalInstruction;

  return voicePrompt.trim();
};


export const useLiveChat = (character: Character, conversation: Conversation) => {
  const [connectionState, setConnectionState] = useState<ConnectionState>('IDLE');
  const [error, setError] = useState<string | null>(null);
  const [currentAmbient, setCurrentAmbient] = useState<AmbientSoundState>(null);
  
  const isMountedRef = useRef(true);
  const sessionPromiseRef = useRef<Promise<any> | null>(null);
  const mediaStreamRef = useRef<MediaStream | null>(null);
  const inputAudioContextRef = useRef<AudioContext | null>(null);
  const outputAudioContextRef = useRef<AudioContext | null>(null);
  const scriptProcessorRef = useRef<ScriptProcessorNode | null>(null);
  const mediaStreamSourceRef = useRef<MediaStreamAudioSourceNode | null>(null);
  const sourcesRef = useRef(new Set<AudioBufferSourceNode>());
  const nextStartTimeRef = useRef(0);
  
  // For ambient sound
  const ambientAudioRefs = useRef<HTMLAudioElement[]>([]);
  const activeAudioIndexRef = useRef<number>(0);
  const fadeIntervalRef = useRef<ReturnType<typeof setInterval> | null>(null);


  useEffect(() => {
    isMountedRef.current = true;
    // Initialize two audio elements for cross-fading
    ambientAudioRefs.current = [new Audio(), new Audio()];
    ambientAudioRefs.current.forEach(audio => {
        audio.loop = true;
    });

    return () => { 
        isMountedRef.current = false; 
    };
  }, []);

  const handleError = useCallback((errorMessage: string, errorObject?: any) => {
    if (errorObject) console.error(`${errorMessage}:`, errorObject);
    else console.error(errorMessage);
  
    let specificError = "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ìž ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.";
    let detailMessage = '';

    if (errorObject instanceof Error) {
        if (errorObject.name === 'NotAllowedError') {
            specificError = "ë§ˆì´í¬ ê¶Œí•œì´ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤. ðŸŽ¤ ë¸Œë¼ìš°ì €ì˜ ì£¼ì†Œì°½ ì˜† ìžë¬¼ì‡  ì•„ì´ì½˜ì„ í´ë¦­í•˜ì—¬ ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš”.";
        } else if (errorObject.name === 'NotFoundError') {
            specificError = "ì—°ê²°ëœ ë§ˆì´í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ðŸŽ§ ë§ˆì´í¬ê°€ ì œëŒ€ë¡œ ì—°ê²°ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.";
        } else {
             detailMessage = errorObject.message;
        }
    } else if (errorObject && typeof errorObject === 'object') {
        if ('reason' in errorObject && typeof errorObject.reason === 'string' && errorObject.reason) {
            detailMessage = `ì—°ê²°ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤: ${errorObject.reason}`;
        } else if ('message' in errorObject && typeof errorObject.message === 'string' && errorObject.message) {
            detailMessage = errorObject.message;
        } else if ('code' in errorObject) {
            detailMessage = `ì—°ê²° ì½”ë“œ: ${errorObject.code}`;
        }
    }

    if (detailMessage) {
        const msg = detailMessage.toLowerCase();
        if (msg.includes('api key not valid') || msg.includes('entity was not found')) {
            specificError = "API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ðŸ”‘ ì„¤ì •ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.";
        } else if (msg.includes('429') || msg.includes('resource_exhausted')) {
            specificError = "API ì‚¬ìš©ëŸ‰ì´ ë„ˆë¬´ ë§ŽìŠµë‹ˆë‹¤. ðŸ“ˆ ìž ì‹œ í›„ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.";
        } else if (msg.includes('deadline expired') || (errorObject?.code === 408)) {
            specificError = "ì—°ê²° ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. â³ ë„¤íŠ¸ì›Œí¬ ì—°ê²° ìƒíƒœë¥¼ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.";
        } else if (msg.includes('internal error') || (errorObject?.code >= 500)) {
            specificError = "ì„œë²„ì— ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ðŸ› ï¸ ìž ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.";
        } else if (msg.includes('service is currently unavailable')) {
            specificError = "ìŒì„± ì±„íŒ… ì„œë¹„ìŠ¤ê°€ í˜„ìž¬ ì¼ì‹œì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. â˜ï¸ ìž ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.";
        } else {
            if (specificError === "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ìž ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.") {
                specificError = detailMessage;
            }
        }
    }
  
    if (isMountedRef.current) {
        setError(specificError);
        setConnectionState('ERROR');
    }
  }, []);

  const fadeAudio = useCallback((
    element: HTMLAudioElement, 
    targetVolume: number, 
    duration: number, 
    onComplete?: () => void
  ) => {
    if (fadeIntervalRef.current) {
        clearInterval(fadeIntervalRef.current);
    }
    const startVolume = element.volume;
    const steps = duration / 20;
    const volumeStep = (targetVolume - startVolume) / steps;
    let currentStep = 0;

    fadeIntervalRef.current = setInterval(() => {
        currentStep++;
        const newVolume = startVolume + (volumeStep * currentStep);
        if ((volumeStep > 0 && newVolume >= targetVolume) || (volumeStep < 0 && newVolume <= targetVolume)) {
            element.volume = targetVolume;
            if (fadeIntervalRef.current) clearInterval(fadeIntervalRef.current);
            onComplete?.();
        } else {
            element.volume = newVolume;
        }
    }, 20);
  }, []);

  const handleAmbientSoundCommand = useCallback((args: any) => {
    const { action, sound: soundKey, volume } = args;
    const targetVolume = Math.max(0, Math.min(1, (volume ?? 30) / 100));

    const activeAudio = ambientAudioRefs.current[activeAudioIndexRef.current];

    switch (action) {
      case 'play': {
        if (!soundKey || !character.ambientSounds?.[soundKey]) {
          console.warn(`Ambient sound key "${soundKey}" not found.`);
          return;
        }
        
        // If same sound is requested, just adjust volume
        if (currentAmbient?.key === soundKey) {
            fadeAudio(activeAudio, targetVolume, 1000);
            setCurrentAmbient(prev => prev ? { ...prev, volume: volume ?? 30 } : null);
            return;
        }

        const inactiveIndex = 1 - activeAudioIndexRef.current;
        const inactiveAudio = ambientAudioRefs.current[inactiveIndex];
        const soundData = character.ambientSounds[soundKey];

        inactiveAudio.src = soundData.url;
        inactiveAudio.volume = 0;
        inactiveAudio.play().catch(e => console.error("Ambient sound autoplay failed:", e));

        fadeAudio(activeAudio, 0, 1500, () => activeAudio.pause());
        fadeAudio(inactiveAudio, targetVolume, 1500);

        activeAudioIndexRef.current = inactiveIndex;
        setCurrentAmbient({ key: soundKey, description: soundData.description, volume: volume ?? 30 });
        break;
      }
      case 'stop': {
        fadeAudio(activeAudio, 0, 1500, () => activeAudio.pause());
        setCurrentAmbient(null);
        break;
      }
      case 'volume': {
        fadeAudio(activeAudio, targetVolume, 1000);
        setCurrentAmbient(prev => prev ? { ...prev, volume: volume ?? 30 } : null);
        break;
      }
    }
  }, [character.ambientSounds, currentAmbient, fadeAudio]);
  
  const closeSession = useCallback(() => {
    if (!isMountedRef.current) return;

    mediaStreamRef.current?.getTracks().forEach(track => track.stop());
    scriptProcessorRef.current?.disconnect();
    mediaStreamSourceRef.current?.disconnect();
    
    sourcesRef.current.forEach(source => { try { source.stop(); } catch (e) { /* ignore */ } });
    sourcesRef.current.clear();
    nextStartTimeRef.current = 0;
    
    ambientAudioRefs.current.forEach(audio => { audio.pause(); audio.src = ''; });
    if (fadeIntervalRef.current) clearInterval(fadeIntervalRef.current);

    sessionPromiseRef.current?.then(session => session?.close()).catch(e => console.error("Error closing session:", e));

    inputAudioContextRef.current?.close().catch(e => console.error("Error closing input audio context:", e));
    outputAudioContextRef.current?.close().catch(e => console.error("Error closing output audio context:", e));
    
    mediaStreamRef.current = null;
    scriptProcessorRef.current = null;
    mediaStreamSourceRef.current = null;
    sessionPromiseRef.current = null;
    inputAudioContextRef.current = null;
    outputAudioContextRef.current = null;

    if (isMountedRef.current) {
      setConnectionState(prev => (prev === 'ERROR' ? 'ERROR' : 'CLOSED'));
    }
  }, []);

  const startSession = useCallback(async () => {
    if (connectionState !== 'IDLE' && connectionState !== 'CLOSED' && connectionState !== 'ERROR') {
      return;
    }

    setConnectionState('CONNECTING');
    setError(null);
    
    try {
      const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });
      mediaStreamRef.current = await navigator.mediaDevices.getUserMedia({ 
        audio: { noiseSuppression: true, echoCancellation: true, autoGainControl: true, }
      });

      inputAudioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: 16000 });
      outputAudioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: 24000 });
      nextStartTimeRef.current = 0;
      
      const voiceSystemPrompt = optimizePromptForVoice(character.systemPrompt, character, conversation);

      sessionPromiseRef.current = ai.live.connect({
        model: 'gemini-2.5-flash-native-audio-preview-09-2025',
        config: {
          systemInstruction: voiceSystemPrompt,
          responseModalities: [Modality.AUDIO],
          speechConfig: { voiceConfig: { prebuiltVoiceConfig: { voiceName: character.voiceName } } },
          tools: [{ functionDeclarations: [controlAmbientSoundFunctionDeclaration] }],
        },
        callbacks: {
          onopen: () => {
            if (!isMountedRef.current) return;
            setConnectionState('CONNECTED');
            if (character.defaultAmbientSound) {
              handleAmbientSoundCommand({ action: 'play', sound: character.defaultAmbientSound, volume: 25 });
            }
          },
          onclose: () => closeSession(),
          onerror: (e) => {
            handleError('Live API Error', e);
            closeSession();
          },
          onmessage: async (message) => {
            try {
              if (message.toolCall) {
                  for (const fc of message.toolCall.functionCalls) {
                      if (fc.name === 'controlAmbientSound') {
                          handleAmbientSoundCommand(fc.args);
                          sessionPromiseRef.current?.then((session) => {
                             session.sendToolResponse({
                                  functionResponses: {
                                      id: fc.id,
                                      name: fc.name,
                                      response: { result: `Ambient sound action '${fc.args.action}' executed.` },
                                  }
                             });
                          });
                      }
                  }
              }
              const base64Audio = message.serverContent?.modelTurn?.parts[0]?.inlineData?.data;
              const outputAudioContext = outputAudioContextRef.current;
              if (base64Audio && outputAudioContext && outputAudioContext.state === 'running') {
                  nextStartTimeRef.current = Math.max(nextStartTimeRef.current, outputAudioContext.currentTime);
                  const audioBuffer = await decodeAudioData(decode(base64Audio), outputAudioContext, 24000, 1);
                  const source = outputAudioContext.createBufferSource();
                  source.buffer = audioBuffer;
                  source.connect(outputAudioContext.destination);
                  source.addEventListener('ended', () => { sourcesRef.current.delete(source); });
                  source.start(nextStartTimeRef.current);
                  nextStartTimeRef.current += audioBuffer.duration;
                  sourcesRef.current.add(source);
              }
               if (message.serverContent?.interrupted) {
                  sourcesRef.current.forEach(source => source.stop());
                  sourcesRef.current.clear();
                  nextStartTimeRef.current = 0;
              }
            } catch (e) {
                handleError('Error processing incoming message', e);
                closeSession();
            }
          },
        },
      });
      
      await sessionPromiseRef.current;

      const inputAudioContext = inputAudioContextRef.current!;
      const source = inputAudioContext.createMediaStreamSource(mediaStreamRef.current);
      mediaStreamSourceRef.current = source;
      
      const scriptProcessor = inputAudioContext.createScriptProcessor(4096, 1, 1);
      scriptProcessorRef.current = scriptProcessor;

      scriptProcessor.onaudioprocess = (audioProcessingEvent) => {
        const inputData = audioProcessingEvent.inputBuffer.getChannelData(0);
        const l = inputData.length;
        const int16 = new Int16Array(l);
        for (let i = 0; i < l; i++) { int16[i] = inputData[i] * 32768; }
        const pcmBlob = { data: encode(new Uint8Array(int16.buffer)), mimeType: 'audio/pcm;rate=16000' };
        
        sessionPromiseRef.current?.then((session) => {
            try { session.sendRealtimeInput({ media: pcmBlob }); } catch(e) { /* ignore */ }
        }).catch(() => { /* ignore */});
      };
      source.connect(scriptProcessor);
      scriptProcessor.connect(inputAudioContext.destination);

    } catch (error) {
      handleError('Failed to start live session', error as Error);
      closeSession();
    }
  }, [character, conversation, closeSession, handleError, handleAmbientSoundCommand, connectionState]);
  
  useEffect(() => {
      return () => {
          closeSession();
      }
  }, [closeSession]);


  return { connectionState, error, startSession, closeSession, currentAmbient };
};
